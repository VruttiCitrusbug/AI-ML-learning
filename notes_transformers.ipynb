{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af900b9-bcbc-4ac1-b65a-3d6f63df75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"The Illustrated Transformer\" \n",
    "# https://chatgpt.com/c/7fb90a99-d3a5-4403-b2b8-ab767a77fbc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c99b2b-876b-4392-8cc6-c1c763f4579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy Ending: '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Imagine you're a wizard and you want to understand a storybook. \n",
    "But this isn't an ordinary storybook. \n",
    "It's filled with words and sentences in a language you don't quite understand yet. \n",
    "That's where the Transformer comes in. It's like a magical spellbook that helps you understand the story.\n",
    "\"\"\"\n",
    "\n",
    "# Here's how it works:\n",
    "\n",
    "\"\"\" Magic Words:\"\"\" \n",
    "# Before you start reading, you need to learn the meanings of different words in the story. \n",
    "# The Transformer helps you do that by turning each word into a special code, kind of like a secret language only you can understand. \n",
    "# These codes are like magic words that help you make sense of the story.\n",
    "\n",
    "# (Imagine each word turning into a special symbol)\n",
    "\n",
    "\"\"\"Finding Connections: \"\"\"\n",
    "# Now, let's say there's a character named \"Bob\" in the story, and you want to know more about him. \n",
    "# The Transformer helps you do that by looking at how all the words in the story are connected.\n",
    "# It finds out which words are important for understanding Bob's role in the story.\n",
    "\n",
    "# (Imagine the book highlighting important words like \"Bob\", \"his\", \"friend\", etc.)\n",
    "\n",
    "\"\"\"Putting it Together: \"\"\"\n",
    "# Once you understand the important parts of the story, the Transformer helps you piece everything together.\n",
    "# It's like putting together a puzzle. \n",
    "# You start with small bits of information and gradually build up a complete picture of what's happening in the story.\n",
    "\n",
    "# (Imagine each layer adding more details to the story until it's complete)\n",
    "\n",
    "\"\"\"Asking Questions: \"\"\"\n",
    "# Sometimes, you might not understand everything in the story, so you ask questions. \n",
    "# The Transformer helps you with that too! It's like having a friendly guide who can explain things you don't understand.\n",
    "\n",
    "# (Imagine asking questions and getting answers from your friendly guide)\n",
    "\n",
    "\"\"\"Happy Ending: \"\"\"\n",
    "# With the help of the Transformer, you finally understand the whole story! \n",
    "# You can now enjoy the adventures of Bob and his friends without any confusion.\n",
    "\n",
    "# (Imagine reaching the end of the story and feeling happy because you understand everything)\n",
    "\n",
    "# So, the Transformer is like the magical guide that helps you understand complex stories by breaking them down into smaller,easier-to-understand parts.\n",
    "# With its help, you can unlock the secrets of any storybook and enjoy the adventures within!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf1baf33-e2f6-4b5c-b6ab-7d385beb3eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input Sentence:'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Input Sentence:'''\n",
    "# \"Hello, how are you?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc4a8a9-6bd1-427f-9719-bca83d867a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLet\\'s visualize the input embeddings for the tokenized words \"Hello\", \",\", \"how\", \"are\", \"you\", and \"?\". \\nFor simplicity, we\\'ll represent each word as a 4-dimensional vector:\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Input Embeddings:'''\n",
    "# input Embeddings: The sentence \"Hello, how are you?\" is tokenized into individual words: [\"Hello\", \",\", \"how\", \"are\", \"you\", \"?\"].\n",
    "# Each word is then converted into a special vector representation, called an embedding.\n",
    "\n",
    "'''\n",
    "Let's visualize the input embeddings for the tokenized words \"Hello\", \",\", \"how\", \"are\", \"you\", and \"?\". \n",
    "For simplicity, we'll represent each word as a 4-dimensional vector:\n",
    "'''\n",
    "\n",
    "# Word      | Embedding Vector\n",
    "# -----------------------------\n",
    "# Hello     | [0.1, 0.2, 0.3, 0.4]\n",
    "# ,         | [0.5, 0.6, 0.7, 0.8]\n",
    "# how       | [0.9, 1.0, 1.1, 1.2]\n",
    "# are       | [1.3, 1.4, 1.5, 1.6]\n",
    "# you       | [1.7, 1.8, 1.9, 2.0]\n",
    "# ?         | [2.1, 2.2, 2.3, 2.4]\n",
    "\n",
    "       # 2.4 |                                 *\n",
    "       #     |                               / |\n",
    "       #     |                             /   |\n",
    "       # 2.0 |                        * /     |\n",
    "       #     |                      /   |     |\n",
    "       #     |                    /     |     |\n",
    "       # 1.6 |               * /       |     |\n",
    "       #     |             /   |       |     |\n",
    "       #     |           /     |       |     |\n",
    "       # 1.2 |      * /       |       |     |\n",
    "       #     |    /   |       |       |     |\n",
    "       #     |  /     |       |       |     |\n",
    "       # 0.8 |*       |       |       |     |\n",
    "       #     |________|_______|_______|_____|_______\n",
    "       #       0.5     0.9     1.3     1.7    2.1   (Dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cc7e15e-65d8-4c78-9621-ae6682a4ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWord      | Embedding Vector | Positional Encoding\\n--------------------------------------------------\\nHello     | [0.1, 0.2, 0.3, 0.4] | [0.2, 0.4, 0.6, 0.8]\\n,         | [0.5, 0.6, 0.7, 0.8] | [0.4, 0.3, 0.2, 0.1]\\nhow       | [0.9, 1.0, 1.1, 1.2] | [0.6, 0.8, 0.2, 0.4]\\nare       | [1.3, 1.4, 1.5, 1.6] | [0.8, 0.6, 0.4, 0.2]\\nyou       | [1.7, 1.8, 1.9, 2.0] | [0.4, 0.2, 0.8, 0.6]\\n?         | [2.1, 2.2, 2.3, 2.4] | [0.9, 0.7, 0.5, 0.3]\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''Positional Encoding:'''\n",
    "\n",
    "# In a Transformer model, each word is represented by a vector, like mentioned. \n",
    "# However, since the Transformer doesn't inherently understand the order of words in a sentence,\n",
    "# we need a way to tell it about the position of each word. That's where positional encoding comes in.\n",
    "\n",
    "# In the sequence \"The cat sat on the mat,\" \n",
    "# positional encoding indicates that \"cat\" comes after \"The\" and before \"sat,\" \"on,\" and \"the.\"\n",
    "\n",
    "'''\n",
    "Word      | Embedding Vector | Positional Encoding\n",
    "--------------------------------------------------\n",
    "Hello     | [0.1, 0.2, 0.3, 0.4] | [0.2, 0.4, 0.6, 0.8]\n",
    ",         | [0.5, 0.6, 0.7, 0.8] | [0.4, 0.3, 0.2, 0.1]\n",
    "how       | [0.9, 1.0, 1.1, 1.2] | [0.6, 0.8, 0.2, 0.4]\n",
    "are       | [1.3, 1.4, 1.5, 1.6] | [0.8, 0.6, 0.4, 0.2]\n",
    "you       | [1.7, 1.8, 1.9, 2.0] | [0.4, 0.2, 0.8, 0.6]\n",
    "?         | [2.1, 2.2, 2.3, 2.4] | [0.9, 0.7, 0.5, 0.3]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c403e046-9a0c-47a2-b90f-71e9bd3a64b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positional Encoding:'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the sentence \"The cat is wet, but a cat doesn't like to be wet,\"\n",
    "# each occurrence of the word \"cat\" would have its own positional encoding vector indicating its position in the sequence.\n",
    "\n",
    "# Here's how it might look:\n",
    "\n",
    "'''Word Embedding Vectors:'''\n",
    "\n",
    "# Each occurrence of \"cat\" would have the same embedding vector, capturing its semantic meaning. \n",
    "# This vector represents the word's features in a high-dimensional space and is the same for every occurrence of \"cat.\"\n",
    "\n",
    "'''Positional Encoding Vectors:'''\n",
    "\n",
    "# Each occurrence of \"cat\" would have a different positional encoding vector indicating its position in the sequence.\n",
    "# For example:\n",
    "# The first occurrence of \"cat\" might have a positional encoding vector indicating its position as the second word in the sequence.\n",
    "# The second occurrence of \"cat\" might have a positional encoding vector indicating its position as the tenth word in the sequence.\n",
    "# By assigning different positional encoding vectors to each occurrence of \"cat,\" \n",
    "# the Transformer model can distinguish between the two instances and understand their positions in the sequence. \n",
    "# This helps the model accurately capture the relationships between words and the sequential structure of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6876ef72-9441-4584-90ff-61c8d532ebe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning Relationships: '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''How actually data learn relationship'''\n",
    "\n",
    "# Imagine you're reading a book about cats, and you come across a sentence that says, \"The cat let out a soft meow.\" \n",
    "# As a human, you understand that \"meow\" is a sound commonly associated with cats. \n",
    "# The Transformer, through self-attention, learns similar associations.\n",
    "\n",
    "# Here's how it works:\n",
    "\n",
    "\"\"\"Self-Attention:\"\"\" \n",
    "# The Transformer looks at each word in the sentence and calculates its importance or relevance in relation to the other words.\n",
    "# When it encounters the word \"cat,\" it pays attention to words nearby, including \"meow\" and \"purr.\"\n",
    "\n",
    "\"\"\"Word Embeddings:\"\"\" \n",
    "# Each word is represented by a vector (embedding) that captures its meaning. \n",
    "# Words like \"cat,\" \"meow,\" and \"purr\" might have similar embeddings because they often appear together in sentences about cats.\n",
    "\n",
    "\"\"\"Attention Scores: \"\"\"\n",
    "# The Transformer calculates attention scores between the word \"cat\" and other words in the sentence, including \"meow\" and \"purr.\" \n",
    "# If the attention scores between \"cat\" and \"meow\"/\"purr\" are high, it indicates a strong relationship.\n",
    "\n",
    "\"\"\"Learning Relationships: \"\"\"\n",
    "# Through training on a large dataset of text, the Transformer learns these relationships. \n",
    "# It discovers that words like \"meow\" and \"purr\" often co-occur with \"cat\" and are likely related to it. \n",
    "# This understanding allows the Transformer to capture the semantic associations between words.\n",
    "\n",
    "# So, by analyzing patterns in the data and learning from examples,\n",
    "# the Transformer can determine that words like \"meow\" and \"purr\" are related to \"cat\" \n",
    "# based on their co-occurrence in sentences and the attention scores calculated during self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c0b7b96-b5c4-4308-997a-2842d3a10a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feedforward Neural Network:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Encoder Stack:'''\n",
    "# Let's break down the Encoder Layers into simpler terms.\n",
    "\n",
    "# Imagine you're a detective solving a mystery. \n",
    "# Each encoder layer is like a magnifying glass that helps you uncover clues and piece together the story.\n",
    "\n",
    "\"\"\"Multi-Head Self-Attention Mechanism:\"\"\"\n",
    "# This is your first tool, the magnifying glass. \n",
    "# It helps you focus on different parts of the mystery.\n",
    "# You look at each clue (word) and pay attention to how it's related to other clues.\n",
    "# For example, if someone mentions \"cat,\" you might pay attention to words like \"meow\" or \"purr.\"\n",
    "\n",
    "\n",
    "'''Feedforward Neural Network:'''\n",
    "# Once you've gathered clues with the magnifying glass, you need to make sense of them. \n",
    "# This is where the feedforward neural network comes in. It's like your brain processing the clues and figuring out what they mean.\n",
    "# You might notice patterns or connections between clues that weren't obvious at first.\n",
    "\n",
    "# Each encoder layer does this detective work, refining your understanding of the mystery step by step. \n",
    "# It's like zooming in on different aspects of the story and gradually piecing together the bigger picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1a4b1-a5db-4a68-b2c5-cb9f316a9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Decoder Stack:'''\n",
    "# Imagine you're a storyteller trying to craft a story based on a picture book. \n",
    "# The Encoder Stack has already looked at the pictures and understood what they represent. \n",
    "# Now, it's your turn to use that understanding to tell the story in words.\n",
    "\n",
    "# Here's how you do it:\n",
    "\n",
    "'''Multi-Head Self-Attention Mechanism:'''\n",
    "\n",
    "# You start by looking at the words you've already written in your story. \n",
    "# Just like you pay attention to what you've already said to keep the story consistent,\n",
    "# the Decoder Stack pays attention to the words it has already generated to ensure coherence.\n",
    "\n",
    "'''Encoder-Decoder Attention Mechanism:'''\n",
    "\n",
    "# Next, you look back at the pictures in the picture book.\n",
    "# You want to make sure your story aligns with what the pictures represent.\n",
    "# Similarly, the Decoder Stack looks back at the output of the Encoder Stack (the understanding of the input)\n",
    "# to ensure that the generated words accurately reflect the meaning of the input sequence.\n",
    "\n",
    "'''Feedforward Neural Network:'''\n",
    "\n",
    "# After considering both the words you've already written and the input pictures, you decide what should happen next in the story. \n",
    "# You use your creativity and understanding to generate the next part of the story. \n",
    "# Similarly, the Decoder Stack processes the information gathered from both attention mechanisms\n",
    "# and makes decisions about what words to generate next in the output sequence.\n",
    "\n",
    "'''Residual Connections and Layer Normalization:'''\n",
    "\n",
    "# Throughout the storytelling prohellocess, you want to ensure that the story flows smoothly and that you don't lose track of the plot.\n",
    "# Residual connections allow you to maintain a consistent flow of information,\n",
    "# while layer normalization ensures that your storytelling remains stable and coherent.\n",
    "\n",
    "# In essence, the Decoder Stack takes the understanding provided by the Encoder Stack and uses it to generate a coherent and accurate output sequence.\n",
    "# It does this by attending to the words it has already generated,\n",
    "# paying attention to the input understanding, processing the information, and ensuring a stable flow of storytelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac01d8ce-704c-4f51-94f7-47379be58088",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # +---------------------------+\n",
    "        # |     Input Sentence        | \n",
    "        # +-------------+-------------+\n",
    "        #               |\n",
    "        # +-------------v-------------+\n",
    "        # |   Input Embeddings        |\n",
    "        # +-------------+-------------+\n",
    "        #               |\n",
    "        # +-------------v-------------+\n",
    "        # |    Positional Encoding    |\n",
    "        # +-------------+-------------+\n",
    "        #               |\n",
    "        # +-------------v-------------+\n",
    "        # |      Encoder Stack        |\n",
    "        # +-------------+-------------+\n",
    "        #               |\n",
    "        # +-------------v-------------+\n",
    "        # |      Decoder Stack        |\n",
    "        # +-------------+-------------+\n",
    "        #               |\n",
    "        # +-------------v-------------+\n",
    "        # |      Output Sequence      |\n",
    "        # +---------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83075c-1baf-4b0c-95ae-e1172ecb5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Models for transformers:\"\"\"\n",
    "\n",
    "# Imagine you have a magic book that can understand stories and tell you new ones. This magic book is called a transformer.\n",
    "\n",
    "'''BERT (Bidirectional Encoder Representations from Transformers):'''\n",
    "# BERT is like a wizard who learns by reading many stories.\n",
    "# But what's cool about BERT is that it doesn't just read stories from beginning to end like you might read a book.\n",
    "# Instead, it reads in both directions, from the start to the middle and from the end to the middle, so it can understand the context really well.\n",
    "\n",
    "'''GPT (Generative Pre-trained Transformer): '''\n",
    "# GPT is another magical book, but it's more about creating stories. \n",
    "# It learns from all the stories it's read and then can make up new ones that sound just like a real person wrote them. \n",
    "# So, if you ask GPT to tell you a story about a dragon and a knight, it will make up a story just for you!\n",
    "\n",
    "'''XLNet: '''\n",
    "# Imagine you have a friend who's really good at solving puzzles. XLNet is like that friend. \n",
    "# It looks at words in a sentence and tries to figure out what comes next by looking at all the words before it, like solving a puzzle. \n",
    "# This helps XLNet understand sentences even better.\n",
    "\n",
    "'''T5 (Text-to-Text Transfer Transformer): '''\n",
    "# T5 is like a magic wand that can turn one kind of text into another.\n",
    "# For example, if you give it a question, it can turn that question into an answer.\n",
    "# Or if you give it a sentence in one language, it can change it into a sentence in another language.\n",
    "\n",
    "'''Transformer-XL: '''\n",
    "# Imagine you're telling a really long story, like a fairy tale that goes on and on. \n",
    "# Transformer-XL is like a memory wizard.\n",
    "# It remembers what happened at the beginning of the story even when you're at the end, so it can understand everything better. \n",
    "# It's like having a friend who remembers all the important details, even from a long time ago."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
